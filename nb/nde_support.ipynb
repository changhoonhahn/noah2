{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train NDE to estimate community distribution $p(X) \\approx \\mathcal{Q}_\\phi(X)$\n",
    "$\\mathcal{Q}_\\phi(X)$ will be used check for support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noah2 import data as D\n",
    "from noah2 import util as U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from nflows import transforms, distributions, flows\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner as DFM\n",
    "# --- plotting ---\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['axes.linewidth'] = 1.5\n",
    "mpl.rcParams['axes.xmargin'] = 1\n",
    "mpl.rcParams['xtick.labelsize'] = 'x-large'\n",
    "mpl.rcParams['xtick.major.size'] = 5\n",
    "mpl.rcParams['xtick.major.width'] = 1.5\n",
    "mpl.rcParams['ytick.labelsize'] = 'x-large'\n",
    "mpl.rcParams['ytick.major.size'] = 5\n",
    "mpl.rcParams['ytick.major.width'] = 1.5\n",
    "mpl.rcParams['legend.frameon'] = False\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): device = 'cuda'\n",
    "else: device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train/valid/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNoah = D.Noah2()\n",
    "fema = DNoah.data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('../dat/nde_support'): \n",
    "    os.system('mkdir -p ../dat/nde_support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ntrain = 158204, Nvalid = 19775, Ntest = 19777\n"
     ]
    }
   ],
   "source": [
    "ftrain = '../dat/nde_support/data.train.npy'\n",
    "fvalid = '../dat/nde_support/data.valid.npy'\n",
    "ftest  = '../dat/nde_support/data.test.npy'\n",
    "\n",
    "if ~(os.path.isfile(ftrain) and os.path.isfile(fvalid) and os.path.isfile(ftest)): \n",
    "\n",
    "    Ntrain = int(0.8 * len(fema))\n",
    "    Nvalid = int(0.1 * len(fema))\n",
    "    Ntest = len(fema) - Ntrain - Nvalid\n",
    "    \n",
    "\n",
    "    irandom = np.arange(len(fema))\n",
    "    np.random.shuffle(irandom)\n",
    "    \n",
    "    x_train = fema[irandom][:Ntrain]\n",
    "    x_valid = fema[irandom][Ntrain:Ntrain+Nvalid]\n",
    "    x_test = fema[irandom][Ntrain+Nvalid:]\n",
    "    \n",
    "    # log10 for income and population \n",
    "    x_train[:,2] = np.log10(x_train[:,2])\n",
    "    x_train[:,3] = np.log10(x_train[:,3])\n",
    "    x_valid[:,2] = np.log10(x_valid[:,2])\n",
    "    x_valid[:,3] = np.log10(x_valid[:,3])\n",
    "    x_test[:,2] = np.log10(x_test[:,2])\n",
    "    x_test[:,3] = np.log10(x_test[:,3])    \n",
    "    \n",
    "    np.save(ftrain, x_train[np.all(np.isfinite(x_train), axis=1)])\n",
    "    np.save(fvalid, x_valid[np.all(np.isfinite(x_valid), axis=1)])\n",
    "    np.save(ftest, x_test[np.all(np.isfinite(x_test), axis=1)])\n",
    "else: \n",
    "    x_train = np.load(ftrain)\n",
    "    x_valid = np.load(fvalid)    \n",
    "    x_test = np.load(ftest)\n",
    "    \n",
    "    Ntrain = x_train.shape[0]\n",
    "    Nvalid = x_valid.shape[0]\n",
    "    Ntest = x_test.shape[0]\n",
    "print('Ntrain = %i, Nvalid = %i, Ntest = %i' % (Ntrain, Nvalid, Ntest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 dimensions\n"
     ]
    }
   ],
   "source": [
    "ndim = x_train.shape[1]\n",
    "print('%i dimensions' % ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cdf transformation of the data\n",
    "The data here have hard ranges. Lets relax this by implementing a CDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [\n",
    "    [-1e-3, 2000.], # mean rainfall\n",
    "    [0., 10.],   # flood risk score\n",
    "    [np.log10(1e4), np.log10(2.5e5)], # log10(median household income)\n",
    "    [np.log10(10), np.log10(2e5)], # log10(population)\n",
    "    [-1e-3, 1.+1e-3], # renter fraction\n",
    "    [-1e-3, 1.+1e-3], # education fraction\n",
    "    [-1e-3, 1.+1e-3] # white fraction\n",
    "]\n",
    "\n",
    "for activity in ['s_c310', 's_c320', 's_c330', 's_c340', 's_c350', 's_c360', 's_c370', \n",
    "                 's_c410', 's_c420', 's_c430', 's_c440', 's_c450', \n",
    "                 's_c510', 's_c520', 's_c530', 's_c540',\n",
    "                 's_c610', 's_c620', 's_c630']: \n",
    "    bounds.append([-1e-3, 100.+1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx_train = np.empty(x_train.shape)\n",
    "cx_valid = np.empty(x_valid.shape)\n",
    "cx_test = np.empty(x_test.shape)\n",
    "\n",
    "for i in range(ndim): \n",
    "    cx_train[:,i] = U.inv_cdf_transform(x_train[:,i], bounds[i])\n",
    "    cx_valid[:,i] = U.inv_cdf_transform(x_valid[:,i], bounds[i])\n",
    "    cx_test[:,i] = U.inv_cdf_transform(x_test[:,i], bounds[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torch.tensor(cx_train.astype(np.float32)).to(device), batch_size=512, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(torch.tensor(cx_valid.astype(np.float32)).to(device), batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train $\\mathcal{Q}_\\phi(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_flow(nhidden, nblocks): \n",
    "    ''' initialize flow with input architecture and standard normal base distribution \n",
    "    '''\n",
    "    blocks = []\n",
    "    for iblock in range(nblocks): \n",
    "        blocks += [transforms.MaskedAffineAutoregressiveTransform(features=ndim, hidden_features=nhidden),\n",
    "                transforms.RandomPermutation(features=ndim)]\n",
    "    transform = transforms.CompositeTransform(blocks)\n",
    "\n",
    "    base_distribution = distributions.StandardNormal(shape=[ndim])\n",
    "    flow = flows.Flow(transform=transform, distribution=base_distribution)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320x8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77495baa79744939a7250b3376cfcaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_iter = 1000\n",
    "patience = 20\n",
    "lrate    = 1e-3\n",
    "\n",
    "all_flows, all_archs, all_valid_losses = [], [], []\n",
    "for i in range(5): \n",
    "    # randomly sample architecture\n",
    "    nhidden = int(np.ceil(np.random.uniform(64, 512)))\n",
    "    nblocks = int(np.random.uniform(3, 15))\n",
    "    print('%ix%i' % (nhidden, nblocks))\n",
    "    \n",
    "    flow = init_flow(nhidden, nblocks) \n",
    "    flow.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(flow.parameters(), lr=lrate)\n",
    "    scheduler = scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, min_lr=1e-5)\n",
    "    #optim.lr_scheduler.OneCycleLR(optimizer, lrate, total_steps=num_iter)\n",
    "\n",
    "    best_epoch, best_valid_loss, valid_losses = 0, np.inf, []\n",
    "    t = trange(num_iter, leave=False)\n",
    "    for epoch in t:\n",
    "        # train \n",
    "        train_loss = 0.\n",
    "        for batch in train_loader: \n",
    "            optimizer.zero_grad()\n",
    "            loss = -flow.log_prob(batch).mean()\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        train_loss = train_loss/float(len(train_loader))\n",
    "    \n",
    "        # validate\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0.\n",
    "            for batch in valid_loader: \n",
    "                loss = -flow.log_prob(batch).mean()\n",
    "                valid_loss += loss.item()\n",
    "            valid_loss = valid_loss/len(valid_loader)\n",
    "            if np.isnan(valid_loss): raise ValueError\n",
    "            valid_losses.append(valid_loss)\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        t.set_description('Epoch: %i LR %.2e TRAINING Loss: %.2e VALIDATION Loss: %.2e' % \n",
    "                          (epoch, scheduler._last_lr[0], train_loss, valid_loss), refresh=False)            \n",
    "            \n",
    "        if valid_loss < best_valid_loss: \n",
    "            best_valid_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "            best_flow = copy.deepcopy(flow)\n",
    "        else: \n",
    "            if epoch > best_epoch + patience: \n",
    "                break \n",
    "\n",
    "    plt.plot(np.arange(len(valid_losses)), valid_losses)\n",
    "    plt.xlim(0, len(valid_losses))\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    all_archs.append('%ix%i' % (nhidden, nblocks))\n",
    "    all_flows.append(best_flow)\n",
    "    all_valid_losses.append(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noah",
   "language": "python",
   "name": "noah"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
